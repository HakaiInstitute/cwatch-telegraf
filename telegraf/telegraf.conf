
# data flow is inputs -> processors -> aggregators -> processors -> outputs
# this means the processors must ignore the sentry packets or they will be filtered out 
[agent]
debug = false

# tail apache access logs and use grok to parse log messages
[[inputs.tail]]
   files = ["/telegraf/log/access.log"]
   from_beginning = false
   grok_patterns = ["%{COMBINED_LOG_FORMAT}"]
   name_override = "apache_access_log"
   # watch_method = "poll"
   watch_method = "inotify"
   path_tag = "log_file_path"
   grok_timezone = "UTC"
   data_format = "grok"
   grok_unique_timestamp="disable"

[[outputs.file]]
  ## Files to write to, "stdout" is a specially handled file.
  files = ["stdout"]
  data_format = "json"
  json_timestamp_units = "1s"

# change data types of fields and tags as needed. fields are the data, tags are used to select/exclude metrics 
[[processors.converter]]
  namedrop = ["sentry"]
  order = 2
  [processors.converter.fields]
    integer = ["resp_bytes"]

  [processors.converter.tags]
    string = ["resp_code"]

# flag log messages that we dont care about.
#   anything from search engine bots
#   any response code that is not in the 200's
#   requests for html, css, or js files
# flag mesages that we do want to include
#  anything going to erddap tabledap or griddap
[[processors.regex]]
  namedrop = ["sentry"]
  order = 3
  [[processors.regex.fields]]
    key = "agent"
    pattern = "^.*(/dataforseo.com/dataforseo-bot|www.semrush.com/bot.html|Koj Bot|yandex.com/bots|www.bing.com/bingbot.htm|webmaster.petalsearch.com/site/petalbot|www.google.com/bot.html|babbar.tech/crawler|webmeup-crawler.com|ahrefs.com/robot|neeva.com/neevabot|opensiteexplorer.org/dotbot/|Slackbot).*"
    replacement = "yes"
    result_key = "exclude"

  [[processors.regex.fields]]
    key = "resp_code"
    pattern = "^2\\d+$"
    replacement = "yes"
    result_key = "include1"

  [[processors.regex.fields]]
    key = "request"
    pattern = "^/erddap/(tabledap|griddap)/[a-zA-Z0-9_]+\\.[a-zA-Z0-9_]+"
    replacement = "yes"
    result_key = "include2"

  [[processors.regex.fields]]
    key = "request"
    pattern = "(\\.css|\\.html|\\.js)(\\?|$)"
    replacement = "yes"
    result_key = "exclude"

# rename some fields so that they are what plausable is expecting
[[processors.rename]]
  namedrop = ["sentry"]
  order = 5
  [[processors.rename.replace]]
    field = "resp_code"
    dest = "code"

  [[processors.rename.replace]]
    field = "request"
    dest = "path"

  [[processors.rename.replace]]
    field = "client_ip"
    dest = "host"

  [[processors.rename.replace]]
    tag = "verb"
    dest = "method"

  [[processors.rename.replace]]
    field = "resp_bytes"
    dest = "size"

# do the actual filtering of messages. 
# also poplate some filds that plausable will need
[[processors.starlark]]
  namedrop = ["sentry"]
  order = 6
  source = '''
load("json.star", "json")
def apply(metric):
  if metric.fields.get('exclude'):
    return None
  if metric.fields.get('include1') == None:
    return None
  if metric.fields.get('include2') == None:
    return None

  metric.fields.pop("include1")
  metric.fields.pop("include2")

  metric.fields['name'] = "File Download"
  metric.fields['domain'] = "${DOMAINS}"
  metric.fields['url'] = "${HOST_URL}" + metric.fields['path']
  metric.fields['host_ip'] = metric.fields['host']
  metric.fields['props'] = json.encode({
    "url": "${HOST_URL}" + metric.fields['path'],
    "download_size": metric.fields['size']
  })

  return metric
'''

# track the last message in a 5 min window. if no log message was received
# in a 5 min window then the aggrigator does not return a message.
[[aggregators.starlark]]
  namepass = ["apache_access_log"]
  name_override = "sentry"
  period = "5m"
  drop_original = false
  source = '''
state = {}
def add(metric):
  metric.tags['status'] = 'ok'
  metric.tags['environment'] = "${ENVIRONMENT}"
  state["last"] = metric

def push():
  return state.get("last")

def reset():
  state.clear()
'''

# send event messages to plausable
[[outputs.execd]]
  namepass = ["apache_access_log"]
  command = ["python3", "-u", "/telegraf/send_http_req.py"]
  data_format = "json"
  json_timestamp_units = "1s"
  json_transformation = '$merge([{"name": name, "timestamp": timestamp}, tags, fields])'

# send last message found in 5 min aggrigation, if any, to sentry to indicate we are still alive.
[[outputs.execd]]
  namepass = ["sentry"]
  command = ["python3", "-u", "/telegraf/send_http_req_sentry.py"]
  data_format = "json"
  json_timestamp_units = "1s"
  json_transformation = '$merge([{"name": name, "timestamp": timestamp}, tags, fields])'


